Satisfaction des usagers et évaluation des politiques publiques
Christine Olm nous entretient sur les outils quantitatifs dans les études et l’évaluation des politiques sociales, et beaucoup sur l’expérimentation sociale ; elle aborde aussi la question des observatoires (dans le contexte de la Métropole) et des enquêtes de satisfaction.Christine Olm, diplômée de l’ENSAE (École Nationale de la Statistique et de l'Administration Économique), sociologue quantitative, spécialiste de l'évaluation des politiques publiques, était jusqu'en 2011 directrice adjointe du département Évaluation des politiques sociales du CRÉDOC (Centre de Recherche pour l’Étude et l'Observation des Conditions de vie)[1], où elle a travaillé une quinzaine d’années sur des problématiques et politiques diversifiées (logement, enfance, insertion des jeunes, RMI/RSA, pauvreté, personnes âgées, handicap et dépendance, accès aux soins, tarification, expérimentations sociales…). Christine Olm codirige aujourd’hui le cabinet VizGet qui réalise des études locales et nationales[2]. 

Quelle est l’utilisation des données quantitatives dans le champ de l’étude, de la conduite et de l’évaluation des politiques sociales ? Elle est très large ! Leur première fonction est de donner accès à la compréhension des territoires, ce qui revient à connaître le contexte socio-économique dans lequel interviennent les politiques, connaître les publics et leurs caractéristiques. Qu’il s’agisse de réaliser une analyse des besoins sociaux, d’évaluer le public éligible à tel ou tel dispositif, de savoir par exemple comment favoriser le retour à l’emploi des femmes qui se sont arrêtées de travailler pour s’occuper de leurs enfants, ou de connaître les caractéristiques des communes de façon à définir des modes de garde, etc., on recourra à l’analyse et l’exploitation secondaire de données produites, notamment par l’Insee qui met à disposition des fichiers détails [3], ou en exploitant les déclarations annuelles des données sociales (DADS). Il est possible d’aller assez loin avec ces données « administratives », avant de faire appel à des méthodologies plus compréhensives.Quelles sont les évolutions perceptibles dans la boîte-à-outils des politiques sociales : méthodologies d’enquêtes, manière d’utiliser les données et les indicateurs, etc. ?  La loi organique relative aux lois de finances (LOLF) a donné une place importante aux indicateurs quantitatifs dans le pilotage des politiques sociales, ce qui à mon sens a fait beaucoup de mal à l’évaluation, en ancrant la confusion entre évaluation et contrôle, et entre évaluation et atteinte des bons résultats. Elle suscite par ailleurs le risque, non négligeable, que les indicateurs finissent par se substituer aux finalités poursuivies par chaque politique publique.Les mesures de satisfaction des usagers sont de plus en plus utilisées pour mesurer la qualité de service dans le champ des politiques sociales. Votre point de vue sur l’utilité de ces mesures ? Ces mesures se justifient, car c’est souvent l’un des objectifs d’une politique publique que de proposer une bonne qualité de service pour les bénéficiaires, et qu’à l’inverse une mauvaise qualité de service est une source d’inefficience. Dans les politiques sociales, c’est un facteur de non-recours aux droits par exemple.A vos yeux, la piste de l’expérimentation sociale est très prometteuse. Quel est son intérêt ? Il est difficile de mesurer ce que l’on appelle les effets propres d’une politique ou d’un dispositif, parce que d'autres facteurs, comme l’environnement socio-économique ou d'autres politiques impactent leur bénéficiaires. Par exemple quelle était l’efficacité du RMI pour ses bénéficiaires, alors que nombre d’entre eux s’installaient dans le dispositif, et qu’on présumait que le dispositif manquait d'incitation à la reprise d'activité ? Les évaluations ex-post montraient l'existence de bénéficiaires restant très longtemps dans le dispositif mais ne suffisaient pas à assurer que c'était un effet du RMI, autrement dit, ne pouvaient répondre à la question de savoir quelle aurait été la trajectoire des bénéficiaires en l'absence du dispositif. L'action publique a tâtonné pour répondre à cette problématique : prime à l'emploi, contrat d'insertion qui se prolonge, dispositifs d'intéressement lors de la reprise d'emploi comme le RSA activité. Autre exemple, les allocations permettant aux parents de rester au foyer (CLCA et APE) ont des risques identifiés : baisse de l'employabilité, stigmatisation supplémentaire des salariées par rapport à leurs collègues masculins, comportements dans le couple où la femme se centre sur les tâches domestiques, mais aucun résultat ne permet de s'assurer que ces risques sont avérés. En matière d’allocations logement, des études ont mesuré leur impact inflationniste sur les loyers selon les zones géographiques, sans que cela ait suffit à mobiliser une action publique. Quels sont les impacts des politiques d'emploi ciblées sur certains publics ? Est-ce que cela modifie simplement l'ordre au sein de files d'attente et donc accroît l'employabilité de certains au détriment d'autres publics ? Face à de telles difficultés, l'expérimentation sociale contrôlée est une solution très intéressante qui permet de mesurer les effets propres des dispositifs de politique publique, et donc de modifier ce qui doit l’être avant de les déployer.Qu’est-ce que l’expérimentation sociale contrôlée, et d’où vient cette méthodologie ? Elle vient des États-Unis. Dans les années 1960, dans le cadre d'un débat sur la protection sociale, des chercheurs ont pensé que l’expérimentation qui fonctionne bien en recherche médicale ou pharmaceutique, à savoir sélectionner aléatoirement, parmi la population cible, un groupe bénéficiaires auquel on donne le traitement, et un groupe témoins auquel on donne un placébo, pour regarder les effets produits par le traitement, devait aussi fonctionner dans le domaine social. Si les effectifs sont suffisants, les paramètres externes susceptibles de biaiser l'analyse sont contrôlés, seul l'effet du dispositif est mesuré. Dans l’expérimentation sociale il y a l’idée de faire à petite échelle, avec un nombre réduit de bénéficiaires, sur une durée limitée, une politique dont on n’est pas sûr qu’elle fonctionne. Elle est mise en œuvre dans des conditions qui permettent d’en évaluer les effets, ce qui permettra de la généraliser, de l'améliorer, ou d'y renoncer... Il faut chaque fois trouver un compromis entre le coût de l'expérimentation et les enseignements qu'il est possible d'en tirer.Pourriez-vous donner des exemples d’expérimentations menées dans le monde ? Ces méthodes sont utilisées dans les pays anglo-saxons, pour évaluer toutes sortes de nouveaux dispositifs, fixation des prix de l'électricité, dispositif de retour à l’emploi, etc. Aux États-Unis en 2000-2001 elles ont servi par exemple à mesurer l’incidence d’informations et de primes sur les décisions d'épargne dans un contexte où les autorités voulaient encourager l’ensemble des salariés à participer au plan de retraite adossé à un compte d'impôt différé, pour éviter la précarité parmi certains seniors.L’expérimentation sociale est-elle utilisée en France ? Oui, mais elle est venue tardivement, en raison de blocages institutionnels (principe d'égalité de l'accès aux droits). Quelques expérimentations ont eu lieu, sur le RMI dans deux départements, sur l'APA, testée dans une douzaine de départements, sur la CMU, qui résulte d'expériences développées par des caisses primaires d'assurance maladie. Mais il a fallu attendre 2003 pour qu’une révision constitutionnelle ouvre le droit à l’expérimentation (toute collectivité locale peut désormais expérimenter sur son territoire une politique, même si elle touche au droit commun et si elle est destinée qu’à une petite partie de la population) et 2005 pour que tout démarre : la commission Familles, vulnérabilité, pauvreté présidée par Martin Hirsch rend son rapport, Au possible, nous sommes tenus. Parmi les mesures proposées, le RSA dont va commencer l’expérimentation contrôlée.Comment s’est déroulée cette expérimentation pionnière sur le RSA ? Martin Hirsch a voulu expérimenter le RSA au sein de territoires témoins, parmi les 33 départements volontaires. Plusieurs modalités ont été expérimentées (champ des bénéficiaires ciblés, types d'emplois et d’accompagnement mis en place, etc.), avec quatre objectifs précis : encourager l’accès ou le retour à l’emploi en garantissant que tout retour à l’emploi donne lieu, dans la durée, à une augmentation de revenus ; lutter contre la pauvreté en complétant les revenus du travail pour ceux qui en ont besoin ; améliorer l’accompagnement social et l’insertion professionnelle ; simplifier le système des minima sociaux. L'expérimentation devait fournir des éléments dans la perspective de généralisation du RSA, et tirer par ailleurs des enseignements pour la démarche d'expérimentation sociale, mais cela ne s’est pas passé comme prévu. L’échantillon témoin aléatoire n’a pas pu être mis en place, parce que les départements ont voulu choisir, sur la base d'une concertation locale, les territoires expérimentaux. Globalement l’expérimentation menée auprès de 3500 allocataires a indiqué un impact positif sur le retour à l'emploi pour les territoires témoins, mais avec des écarts importants selon les départements, ce qui signalait l’influence des modalités de mise en œuvre et des organisations locales.Martin Hirsch semble avoir eu une influence décisive dans la promotion de ces méthodologies ? Oui, en tout cas pour la France. Il portait l’idée qu’on peut faire au niveau local, en tirant partie des bonnes idées et des énergies, ce qui ne peut être réalisé au niveau national. En 2007, la Délégation interministérielle à l’innovation, à l’expérimentation sociale et l’économie sociale (DIIESES) a lancé un premier appel à projets d'innovation sociale (37 projets seront financés). L’année suivante, c’est aussi Martin Hirsch qui crée le Fonds d'Expérimentation pour la Jeunesse (FEJ), très bien doté, pour promouvoir l'innovation sociale, et se donner les moyens d'identifier les projets qui ont un impact positif pour construire des politiques nationales. Toute association et collectivité pouvait en réaliser, à condition de les accompagner d’une évaluation. Une multitude de projets et d’expérimentations ont alors éclos au niveau local, sur les politiques jeunesse et sur d’autres thématiques (égalité homme-femme, etc.) alors même que ni les porteurs de projet, ni les évaluateurs, n’avaient de culture de l'expérimentation sociale contrôlée. Aujourd’hui les ministères soutenant l'expérimentation sont plus nombreux, et plusieurs centaines de millions d'euros y sont dédiées.Quelles ont été les expérimentations phares en France ? Une expérimentation d’envergure a consisté à comparer un dispositif d’accompagnement renforcé, dont l’objectif était d’accélérer le retour à l'emploi des demandeurs d’emploi, porté par deux programmes : Cap Vers l’Entreprise (CVE), directement mis en œuvre par l’ANPE, et Opérateurs Privés de Placement (OPP), service fourni par des entreprises privées mandatées par l’UNEDIC. Menée dans les règles de l’art, elle a porté sur 40 000 bénéficiaires, accompagnés par des OPP dans 10 régions, et par l'ANPE dans 6 régions. Les actions étaient identifiables : accompagnement hebdomadaire, durant 6 mois, par des conseillers ayant en charge peu de demandeurs d’emploi, public cible bien défini : des demandeurs d'emploi susceptibles d'être concernés par du chômage de longue durée. Un évaluateur externe compétent (laboratoire CREST) a été mobilisé. Même s’il y a eu quelques difficultés (moins de la moitié des personnes orientées ont accepté d'entrer dans le programme, d’où un groupe bénéficiaires non strictement comparable au groupe témoins), on a obtenu des résultats scientifiques rigoureux. Les deux dispositifs (le CVE comme l'accompagnement par les OPP) ont eu un impact positif sur les sorties du chômage et sur celles des situations d'activité réduite. Ils ont diminué la récurrence au chômage. Le CVE a eu un impact à la fois plus important et plus rapide, sur la sortie du chômage, ce qui a suscité des interrogations en termes d'efficience, l'accompagnement par les OPP étant très coûteux.Le recul depuis 2007 sur l’expérimentation sociale a-t-il permis de progresser ? En France, nous avons véritablement « expérimenté l’expérimentation », et beaucoup débattu entre professionnels sur ce qu’était l’expérimentation sociale à la française, qui a une dimension plus qualitative que dans les pays anglo-saxons. Pour fournir des résultats scientifiquement incontestables, une expérimentation doit être évaluée de manière externe, doit être évaluable (la politique expérimentale doit avoir des objectifs précis, mesurables…), la population cible doit être suffisamment nombreuse pour permettre de se prononcer sur les différences observées entre les groupes témoins et les groupes bénéficiaires. Ces méthodes soulèvent pour autant de nombreuses questions. : faut-il par exemple privilégier, comme le veut la théorie, les expérimentations contrôlées randomisées ? Ces expérimentations restent-elles valides si elles ne sont pas contrôlées, autrement dit sans échantillon témoin, sachant que sept fois sur dix, pour des raisons diverses, c’est le cas. En pratique, je dirais que cela n’empêche pas d’avoir des résultats intéressants[4]. On s’est aussi rendu compte qu’il était préférable de privilégier l'essaimage plutôt que la généralisation immédiate des dispositifs expérimentés.Est-il aisé d’analyser les résultats d’une expérimentation sociale ? Non, car on se heurte à plusieurs types de difficultés : risques de contamination entre les échantillons témoins et bénéficiaires (la présence de l'expérimentation peut avoir un impact sur la situation de l'échantillon témoins) ; diminution de la participation aux enquêtes au cours du temps, ce qui complexifie l'interprétation des résultats ; temps de l’expérimentation qui est parfois différent du temps politique, avec des porteurs de projet qui veulent obtenir un financement pour un projet qu'ils estiment utile pour leur territoire, et font alors pression sur l'évaluation pour obtenir de manière trop précoce des résultats... La question de stabiliser le financement à l'issue du temps de l'expérimentation se pose aussi.Quelles sont les autres difficultés de mise en œuvre de l’expérimentation sociale ?  Elles sont à plusieurs niveaux. Au moment de l’évaluation, dire à des travailleurs sociaux qu’on va choisir au hasard des bénéficiaires passe très mal, cela ne cadre pas avec leurs schémas : l'égalité de traitement est un principe très ancré dans le secteur social, et s’il y a sélection, on voudra l’orienter vers « les bénéficiaires qui en ont le plus besoin ». Lors de l’expérimentation dans sept missions locales d’un parcours vers l’apprentissage, les structures estimaient qu’elles n’avaient pas à aider des jeunes qui ont un Bac+2 et des parents cadres, alors qu’en réalité ils pouvaient avoir besoin d’une aide, et que l’expérimentation sociale cherchait à connaître l’effet du dispositif « quel que soit le jeune », donc il était important de ne pas se polariser sur une catégorie donnée. De telles représentations sont un frein aux expérimentations, d’autant que s’y ajoute souvent un frein politique dans le même sens. Dans le champ de la santé, la randomisation est la normalité, pas dans le champ social. Pour pallier aux réticences, je fais valoir que le hasard est intéressant, par sa capacité à révéler des publics et des besoins jamais identifiés, mais cela suffit rarement. Nous mettons alors en place des « jokers » : les structures évaluées pourront décider par exemple de sortir de l’évaluation certaines personnes, ce qui ne fausse guère les résultats et permet de trouver un terrain d’entente. Des aménagements et négociations sont donc nécessaires pour faire accepter le principe de la randomisation.A partir de votre expérience des observatoires, quelles seraient les évolutions possibles pour la Métropole de Lyon dans ce domaine ?  Ma conviction est qu’il serait intéressant de créer un observatoire dans une logique transverse aux politiques de la collectivité, et non dans une logique sectorielle. Pour réaliser ses politiques, le Grand Lyon élabore de nombreux programmes d’action, qui forcément se marchent un peu les uns sur les autres, ce qui pourrait être davantage le cas avec les compétences prises au 1er janvier 2015 par la Métropole. La mise en place d’un observatoire transversal serait un moyen de produire de la cohérence en amont, à partir d’un travail sur les indicateurs de suivi des politiques menées, et en aval en montrant les points de croisement entre ces plans et programmes d’action. Un tel observatoire transversal n’existe pas à ma connaissance, ce qui existe, ce sont des observatoires locaux thématiques (social, habitat, etc.), qui fonctionnent bien au demeurant, mais selon une autre logique. L’idée ne serait pas forcément de les fusionner, mais de préciser l’échange de données, dans les deux sens, entre l’observatoire transverse et les observatoires thématiques.Concrètement, comment mettre en place un tel observatoire ?  Il est possible de l’alimenter tous les ans en mobilisant des données disponibles, puis en les complétant par des données plus spécifiques. Le cadre de transmission des données fournies chaque année serait formalisé, de manière à « encapsuler » les indicateurs jugés pertinents pour l’observatoire. C’est ce que nous réalisons à VizGet pour le suivi des salariés de l’hospitalisation privée. Le premier travail est de préciser ou redéfinir avec le commanditaire les objectifs de sa ou de ses politiques, pour savoir ce qui sera évalué en continu. Si l’on s’intéresse à l’efficience et la cohérence des politiques, on aura à la fois des données sur les bénéficiaires et sur les professionnels qui les mettent en œuvre. Pour définir les indicateurs, il convient ensuite d’avoir une idée précise des données existantes, donc de rencontrer les producteurs de données, par exemple les observatoires existants dans le cas du Grand Lyon. On se rendra alors compte qu’il y a des « trous », qu’il est possible ou non de combler. Dans tous les cas, il convient de partir de l’existant, de prendre connaissance des tableaux de bord que les personnes remplissent déjà, et de s’interroger sur l’information vraiment utile que l’on sollicitera.Quelles seraient les erreurs à ne pas commettre pour construire ces d’indicateurs de suivi des politiques ? Déjà, il est impératif de se conformer au principe déontologique de la confidentialité des données. Tout observatoire doit utiliser des données anonymisées, c’est fondamental, d’autant que l’enjeu de protection des données personnelles n’a jamais été aussi important (tendance au rapprochement des données, etc.).