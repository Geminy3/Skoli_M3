La place du robot comme outil de recherche


 Entretien réalisé par Geoffroy Bing, le 28 janvier 2011L'équipe IMAGINE (Extraction de Caractéristiques et Identification) du laboratoire LIRIS réunit 14 enseignants-chercheurs de l'INSA, de l'université Lyon 1, de l'ECL et de l'université Lyon 2. Le métier de base de ces permanents est l'analyse et le traitement des médias visuels. L'équipe a pour objectif d'analyser le contenu de ces médias, notamment images, vidéos, objets 3D et images de documents dans le but d'une meilleure compréhension et interprétation du contenu. L’équipe s’est dotée récemment d’un robot pour explorer de nouveaux champs de la vision dans des contextes plus réalistes et plus complexes. Christian Wolf, maître de conférences au sein de l’équipe, nous explique la place du robot comme outil de recherche et nous fait toucher du doigt l’enjeu de la vision dans l’intelligence des robots de demain.Dans vos recherches sur la vision, à quoi vous sert un robot ?Nous ne nous intéressons pas à la robotique en soi. Pour nous, c'est une application qui nous permet de réaliser nos recherches sur la vision dans un cadre très réaliste. Le robot dispose en effet de capteurs qui viennent enrichir l’image en lui apportant des informations supplémentaires. Le robot est pour nous une source d’images très intéressantes car contextualisées. Le cœur de métier de notre équipe, c'est le renseignement par l’image. Nous concevons des systèmes intelligents capables d’interpréter ces images, d’en extraire des caractéristiques puissantes pour que l'on puisse livrer ces caractéristiques à un module qui gère l'indépendance. Pour vous, qu’est-ce qu’un robot ?C'est une entité mécanique indépendante et qui réagit toute seule. Quelles que soient les applications, votre robot a besoin de l'information lui permettant de lire la scène dans laquelle il se trouve. Dans le domaine de la santé par exemple, il faudra que le robot détecte où se trouve la personne dont il doit s'occuper, déterminer si cette personne est debout ou couchée et, si elle est parterre, si elle l'est volontairement ou si elle est tombée, etc. Où en est-on aujourd’hui dans le champ de la vision ?Aujourd'hui, si vous voulez reconnaître des objets, il faut d'abord construire une base de données dans laquelle se trouvent Ces objets. Il faut d'abord apprendre au système ce qu'est un meuble, un cintre ou une chaise par exemple. On peut avoir des algorithmes qui procèdent par classification et sont capables de reconnaître n'importe quelle chaise ou table dans certaines limites de ressemblance avec un objet de référence. De même que nous travaillons sur la reconnaissance d’objet, d’autres membres de notre équipe travaillent sur la reconnaissance des visages et des émotions. Concrètement, que faîtes-vous avec votre robot ?Nous venons d'acquérir un robot, un Peakee2 d'une entreprise de Montpellier. Notre objectif est de le balader dans un environnement intérieur et de lui faire détecter les visages, les objets et les comportements qu'on lui a appris à reconnaître. Il doit être capable de dire si deux personnes se serrent la main, si un individu court ou marche. Par contre, nous ne sommes pas concernés par ce que fait le robot avec ces informations. L’intelligence du robot est donc en grande partie dictée par sa capacité à lire et interpréter son environnement. Quels sont les principaux freins à lever dans ce domaine ?Les algorithmes capables de détecter une personne qui chute existent déjà. Par contre, ces algorithmes sont robustes dans un intervalle de paramètres qui n’est pas toujours très large. Prenez la même personne qui tombe dans un environnement moins lumineux que celui  dans lequel le robot a été programmé, et votre robot ne détectera pas forcément la chute. Notre défi, et c'est la raison pour laquelle nous nous sommes lancés dans la robotique, c'est de faire marcher nos algorithmes sur un robot alors qu'auparavant ils fonctionnaient dans un environnement stable. Le robot, au contraire, va permettre de tester nos algorithmes dans un environnement variable. Ces freins éloignent-ils les perspectives de développement des robots d’assistance à la personne ?Dans le domaine de l’assistance, il n'y a pas 3000 choses qui peuvent arriver dans une maison. Si l'on se focalise sur quelques cas (chute de la personne, saignement, etc.), il est possible de développer des robots capables de lire et d’interpréter ces cas. Il faut juste rendre le système le plus robuste possible. Si le robot voit quelque chose qui ne figure pas dans sa base d'apprentissage, il ne la reconnaitra pas. Pour les événements dits rares, la méthode consiste à faire tourner un algorithme pendant longtemps afin d'être face à un échantillon représentatif de tout ce que le robot pourrait voir. Et si par la suite le robot voit quelque chose qu'il n'a pas encore vu, il donne l'alerte (sans pour autant qu'il puisse dire ce que c'est...). C'est utilisé pour la sécurité dans les aéroports ou les grandes surfaces sous la forme de caméras intelligentes par exemple. Est-on encore loin de doter les robots d’une intelligence artificielle ?Oui, on n’en est encore loin. Par exemple, à ma connaissance aucun système n’a encore remporté le test de Turing (voir encadré en fin d’entretien). Il n’y a pas de système intelligent et je pense qu’il n’Y en aura pas dans les 20-30 prochaines années. Par contre, dans des tests de Turing restreints, c’est-à-dire circonscrits à un certain contexte, par exemple le médical, il y a des systèmes très performants qui se développent : sur une pathologie donnée, le médecin peut interagir avec un robot sans s’apercevoir qu’il s’agit d’un robot. En revanche, ce robot ne pourra pas prendre du recul par rapport à ce contexte : demandez-lui s’il veut aller au restaurant ce soir et il ne vous répondra pas ! La vraie intelligence artificielle n’existe pas et, heureusement, elle n’est pas nécessaire pour un bon nombre d’applications robotiques. On peut voir émerger la robotique de service sans y intégrer de l’intelligence artificielle ?Parfaitement. Prenez par exemple le robot aspirateur. C’est un robot très stupide mais qui marche très bien. Il ne se pose pas la question de savoir où il est, ni où il doit aller ! Il va, c'est tout. L'algorithme de ce robot est très simple : il dirige aléatoirement le robot et lui fait changer de direction s'il rencontre un obstacle. Statistiquement, après un temps donné selon la surface, il n'y a plus un seul recoin où il n'est pas allé. En comparaison, la partie vision est capable de choses que les autres capteurs ne peuvent pas faire (par exemple le capteur de distance vous donnera juste la distance tandis que la vision donnera tout un ensemble d’autres informations) mais il est beaucoup plus difficile de s'en servir. On ne se rend pas compte de l'importance du cerveau dans le champ de la vision. Entre une photo prise en pleine lumière ou à contre-jour, ce n'est pas du tout le même algorithme qu'il y a derrière. Pour la première, l'algorithme composera à partir de la couleur, de la texture. Pour la deuxième, il composera avec les formes. Ce ne sont pas du tout les mêmes calculs ! Peut-on attendre ces prochains temps des sauts technologiques majeurs dans le domaine de la vision ?J’ai l’impression que c’était la vision qui était un peu le frein de la robotique parce que pour faire des choses intéressantes, il faut une très grande puissance de calcul. Mais l’arrivée sur le marché de technologies comme la Kinect témoignent des grands progrès réalisés, et ce n’est certainement pas fini. Je vois beaucoup d’applications dans le secteur industriel. Ce qui marche bien aujourd'hui, c'est par exemple l'inspection industrielle, c'est-à-dire des robots capables de détecter des défauts sur des pièces industrielles. Ils agissent dans un environnement parfaitement maîtrisé (l'éclairage, la vitesse de défilement des pièces, etc.). Demain, nous verrons peut-être apparaître des robots industriels plus polyvalents, capables d’intervenir sur des petites séries en coopération avec l’homme ou de déplacer des charges lourdes de façon autonome. Mais il faudra compter avec les problèmes de sécurité que cela pose. Avec qui travaillez-vous dans la région ?Nous travaillons en étroite collaboration avec Imaginove, bien qu’un récent projet commun sur des applications robotique de surveillance n'ait malheureusement pas été retenu par l'ANR. Grenoble possède l’un des tous premiers pôles optiques de France et nous connaissons bien l’INRIA et le Gipsa-Lab. Notre laboratoire a également essaimé dans la région avec plusieurs start-ups comme Foxstream sur la videosurveillance ou Corenum dans l’analyse de documents numériques. Demain, notre recherche pourra tout à fait être valorisée dans la robotique. Si le Grand Lyon se décide à faire un cluster robotique, nous serons certainement candidat pour en faire partie ! Test de TuringLe test de Turing est une proposition de test d’intelligence artificielle ayant la faculté d’imiter la conversation humaine. Décrit par Alan Turing en 1950 dans sa publication Computing machinery and intelligence, ce test consiste à mettre en confrontation verbale un humain avec un ordinateur et un autre humain à l’aveugle. Si l’homme qui engage les conversations n’est pas capable de dire qui est l’ordinateur et qui est l’autre homme, on peut considérer que le logiciel de l’ordinateur a passé avec succès le test 